---
title: "Random Forest vs PLS on Random Data"
date: 2015-12-12
categories: [random-forest, machine-learning, partial-least-squares, statistics, analysis]
subtitle: |
  Comparing random-forest and partial-least-squares discriminant-analysis on random data to show the problems inherent in PLS-DA.
---

## TL;DR

Partial least squares (PLS) discriminant-analysis (DA) can ridiculously over fit
even on completely random data. The quality of the PLS-DA model can be assessed
using cross-validation, but cross-validation is not typically performed in many
metabolomics publications. Random forest, in contrast, because of the *forest* of
decision tree learners, and the out-of-bag (OOB) samples used for testing each tree, 
automatically provides an indication of the quality of the model.

## Why?

I've recently been working on some machine learning work using **random forests**
(RF) [Breimann, 2001](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) on metabolomics data. This has been relatively successful,
with decent sensitivity and specificity, and hopefully I'll be able to post more
soon. However, PLS (Wold, 1975) is a standard technique used in metabolomics
due to the prevalence of analytical chemists in metabolomics and a long familiarity
with the method. Importantly, my collaborators frequently use PLS-DA to generate
plots to show that the various classes of samples are separable.

However, it has long been known that PLS (and all of it's variants, PLS-DA, OPLS,
OPLS-DA, etc) can easily generate models that over fit the data, and that over fitting
of the model needs to be assessed if the model is going to be used in subsequent
analyses. 

## Random Data

To illustrate the behavior of both RF and PLS-DA, we will generate some random data
where each of the samples are randomly assigned to one of two classes.

### Feature Intensities

We will generate a data set with 1000 features, where each feature's mean value
is from a uniform distribution with a range of 1-10000.

```{r initial_values}
library(ggplot2)
theme_set(cowplot::theme_cowplot())
library(fakeDataWithError)
set.seed(1234)
n_point <- 1000
max_value <- 10000
init_values <- runif(n_point, 0, max_value)
```

```{r plot_initial}
init_data <- data.frame(data = init_values)
ggplot(init_data, aes(x = data)) + geom_histogram() + ggtitle("Initial Data")
```

For each of these features, their distribution across samples will be based on
a random normal distribution where the mean is the initial feature value and a
standard deviation of 200. The number of samples is 100.

```{r add_error}
n_sample <- 100
error_values <- add_uniform_noise(n_sample, init_values, 200)
```

Just for information, the `add_uniform_noise` function is this:

```{r add_noise_func}
add_uniform_noise
```

I created it as part of a package that is able to add different kinds of noise
to data.

The distribution of values for a single feature looks like this:

```{r plot_error}
error_data <- data.frame(feature_1 = error_values[1,])
ggplot(error_data, aes(x = feature_1)) + geom_histogram() + ggtitle("Error Data")
```

And we will assign the first 50 samples to **class_1** and the second 50 samples
to **class_2**.

```{r sample_classes}
sample_class <- rep(c("class_1", "class_2"), each = 50)
sample_class
```

## PCA

Just to show that the data is pretty random, lets use principal components
analysis (PCA) to do a decomposition, and plot the first two components:

```{r pca_data}
tmp_pca <- prcomp(t(error_values), center = TRUE, scale. = TRUE)
pca_data <- as.data.frame(tmp_pca$x[, 1:2])
pca_data$class <- as.factor(sample_class)
ggplot(pca_data, aes(x = PC1, y = PC2, color = class)) + geom_point(size = 4)
```


## Random Forest

Let's use RF first, and see how things look.

```{r rf_assess}
library(randomForest)
rf_model <- randomForest(t(error_values), y = as.factor(sample_class))
```

The confusion matrix comparing actual *vs* predicted classes based on the 
out of bag (OOB) samples:

```{r rf_out}
knitr::kable(rf_model$confusion)
```

And an overall error of `r mean(rf_model$err.rate)`.

## PLS-DA

So PLS-DA is really just PLS with **y** variable that is binary.

```{r do_plsda}
library(caret)
pls_model <- plsda(t(error_values), as.factor(sample_class), ncomp = 2)
pls_scores <- data.frame(comp1 = pls_model$scores[,1], comp2 = pls_model$scores[,2], class = sample_class)
```

And plot the PLS scores:

```{r plot_plsda}
ggplot(pls_scores, aes(x = comp1, y = comp2, color = class)) + geom_point(size = 4) + ggtitle("PLS-DA of Random Data")
```

And voila! Perfectly separated data! If I didn't tell you that it was random, would
you suspect it?

## Cross-validated PLS-DA

Of course, one way to truly assess the worth of the model would be to use
cross-validation, where a fraction of data is held back, and the model trained
on the rest. Predictions are then made on the held back fraction, and because we
know the truth, we will then calculate the **area under the reciever operator curve**
(AUROC) or area under the curve (AUC) created by plotting true positives *vs* 
false positives.

To do this we will need two functions:

1. Generates all of the CV folds
2. Generates PLS-DA model, does prediction on hold out, calculates AUC

```{r cv_plsda}
library(cvTools)
library(ROCR)

gen_cv <- function(xdata, ydata, nrep, kfold){
  n_sample <- length(ydata)
  all_index <- seq(1, n_sample)
  cv_data <- cvFolds(n_sample, K = kfold, R = nrep, type = "random")
  
  rep_values <- vapply(seq(1, nrep), function(in_rep){
    use_rep <- cv_data$subsets[, in_rep]
    cv_values <- vapply(seq(1, kfold), function(in_fold){
      test_index <- use_rep[cv_data$which == in_fold]
      train_index <- all_index[-test_index]
      
      plsda_cv(xdata[train_index, ], ydata[train_index], xdata[test_index, ],
               ydata[test_index])
    }, numeric(1))
  }, numeric(kfold))
}

plsda_cv <- function(xtrain, ytrain, xtest, ytest){
  pls_model <- plsda(xtrain, ytrain, ncomp = 2)
  pls_pred <- predict(pls_model, xtest, type = "prob")
  
  use_pred <- pls_pred[, 2, 1]
  
  pred_perf <- ROCR::prediction(use_pred, ytest)
  pred_auc <- ROCR::performance(pred_perf, "auc")@y.values[[1]]
  return(pred_auc)
}
```

And now lets do a bunch of replicates (100).

```{r rep_plsda}
cv_vals <- gen_cv(t(error_values), factor(sample_class), nrep = 100, kfold = 5)

mean(cv_vals)
sd(cv_vals)

cv_frame <- data.frame(auc = as.vector(cv_vals))
ggplot(cv_frame, aes(x = auc)) + geom_histogram(binwidth = 0.01)
```

So we get an average AUC of `r mean(cv_vals)`, which is pretty awful. This implies
that even though there was good separation on the scores, maybe the model is
not actually that good, and we should be cautious of any predictions being made.

Of course, the PCA at the beginning of the analysis shows that there is no *real*
separation in the data in the first place. 

```{r session_info}
devtools::session_info()
```

