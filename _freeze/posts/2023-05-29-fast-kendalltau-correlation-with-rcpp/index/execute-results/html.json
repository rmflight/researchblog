{
  "hash": "c7248543f88c313bef24e87e80dd24f6",
  "result": {
    "markdown": "---\ntitle: \"Fast Kendall-tau Correlation with Rcpp\"\nsubtitle: |\n  How we created a fast implementation of Kendall-tau using Rcpp\ndate: 2023-05-29\ncategories: [c++, R, packages,]\neditor_options:\n  chunk_output_type: console\nbibliography: refs.bib\n---\n\n\n## TL;DR\n\nCheck out our `{ICIKendallTau}` R package if you want access to a fast version of Kendall-tau correlation in R [@iciktflight] with only a few dependencies.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# remotes::install_github(\"moseleybioinformaticslab/icikendalltau)\n# install.packages(\"microbenchmark\")\nlibrary(ICIKendallTau)\n```\n:::\n\n\n## Kendall Tau??\n\nYes, Kendall-tau correlation!\nIt is a rank based correlation based on the number of concordant and discordant **pairs** of points.\nThis graphic from Wikipedia explains it really well [@kendallwikipedia].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::include_graphics(\"Concordant_Points_Kendall_Correlation.svg\")\n```\n\n::: {.cell-output-display}\n![All points in the gray area are concordant and all points in the white area are discordant with respect to point X1, Y1. With n = 30 points, there are a total of 435 possible point pairs. In this example there are 395 concordant point pairs and 40 discordant point pairs, leading to a Kendall rank correlation coefficient of 0.816. [@kendallfigure]](Concordant_Points_Kendall_Correlation.svg){#fig-wikipedia-points}\n:::\n:::\n\n\nThis is a really useful correlation to use if you don't want to have to worry about how linearly related things are, just whether the points from two samples go in the same direction or not.\nIn addition, we think there is a neat variant we can make to incorporate the presence of missing values when they are missing not at random, and we have a preprint on that that I am working on revising [@flightInformationContentInformedKendalltauCorrelation2022a].\n\n## Need for Speed!\n\nHowever, there is an issue with the basic Kendall-tau algorithm.\nIt is slower than molasses going uphill in January (as my parents used to say).\nEspecially as we increase to correlations calculated using tens of thousands of features in both *x* and *y*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_large = rnorm(10000)\ny_large = rnorm(10000)\nmicrobenchmark::microbenchmark(cor(x_large, y_large, method = \"kendall\"),\n                               times = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: seconds\n                                      expr      min       lq     mean   median\n cor(x_large, y_large, method = \"kendall\") 1.086599 1.104935 1.146724 1.127946\n       uq      max neval\n 1.137544 1.417044    20\n```\n:::\n:::\n\n\nThat took a full second!\nAnd the size of things we frequently want to calculate for eukaryotic based transcriptomics are 2X - 4X that size, and across many, many samples.\n\nSo if we can speed it up, that would be awesome.\n\nI will note through all of this work, that I've already been through this once in the development of our `{ICIKendallTau}` package, so I already know the answer.\nHowever, I felt it would be useful to work through all of this again to help others who might be looking at similar types of problems.\nAnd yes, some of the below code seems silly, but they are first stabs at an implementation.\n\n## Differences of Signs\n\nThe simplest way to find the concordant and discordant pairs is to generate all the possible pair indices of the points, and then compare their directions; the same direction of a pair in both X and Y means they are concordant, and different directions means they are discordant.\n\nAnd in fact, that is exactly what is happening in the C code for R's covariance / correlation code, iterating over each pair of points (`k` and `n1`; snippet shown here from lines 108 - 118 of file src/library/stats/src/cov.c in R-4.3.0).\n\n```c\nelse { /* Kendall's tau */                      \\\n    for(n1=0 ; n1 < k ; n1++)                   \\\n        if(!(ISNAN(xx[n1]) || ISNAN(yy[n1]))) { \\\n            xm = sign(xx[k] - xx[n1]);          \\\n            ym = sign(yy[k] - yy[n1]);          \\\n                                                \\\n            COV_SUM_UPDATE                      \\\n        }                                       \\\n}                                               \\\n```\n\n### R Based, Copy All Pairs\n\nWhat can we do in R that is similar?\nWe can generate all the pairwise indices, create actual vectors, and then get the signs of the differences maybe?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreference_ici = function(x, y)\n{\n  n_x = length(x)\n  n_y = length(y)\n  pairs = combn(n_x, 2)\n  x_pairs = rbind(x[pairs[1, ]],\n                  x[pairs[2, ]])\n  y_pairs = rbind(y[pairs[1, ]],\n                  y[pairs[2, ]])\n  x_sign = sign(x_pairs[1, ] - x_pairs[2, ])\n  y_sign = sign(y_pairs[1, ] - y_pairs[2, ])\n  x_y_sign = x_sign * y_sign\n  sum_concordant = sum(x_y_sign > 0)\n  sum_discordant = sum(x_y_sign < 0)\n  \n  x_is_dup = duplicated(x)\n  x_dup = x[x_is_dup]\n  x_tied_values_t1 = table(x_dup) + 1;\n  y_is_dup = duplicated(y)\n  y_dup = y[y_is_dup]\n  y_tied_values_t2 = table(y_dup) + 1\n  \n  x_tied_sum_t1 = sum(x_tied_values_t1 * (x_tied_values_t1 - 1)) / 2\n  y_tied_sum_t2 = sum(y_tied_values_t2 * (y_tied_values_t2 - 1)) / 2\n  t_0 = n_x * (n_x - 1) / 2\n  \n  k_denominator = sqrt((t_0 - x_tied_sum_t1) * (t_0 - y_tied_sum_t2))\n  k_numerator = sum_concordant - sum_discordant\n  \n  k_tau = k_numerator / k_denominator\n  \n  k_tau\n}\n```\n:::\n\n\nLet's see how long this takes as a baseline, and how it compares to the `{stats::cor}` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nx = rnorm(1000)\ny = rnorm(1000)\nmicrobenchmark::microbenchmark(cor(x, y, method = \"kendall\"),\n                               reference_ici(x, y),\n                               times = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: milliseconds\n                          expr       min        lq      mean    median\n cor(x, y, method = \"kendall\")  11.44744  12.47315  12.84324  13.07735\n           reference_ici(x, y) 351.74901 368.20867 392.19704 387.35215\n        uq       max neval\n  13.28316  13.63687    20\n 417.69942 432.55782    20\n```\n:::\n:::\n\n\nNot so great.\nNot that surprising, given the main correlation algorithm in R is written in C.\nLet's see if we can speed that up.\nAlthough our code **seems** vectorized here, there is significant time taken in creating the large matrices to hold the pair indices, and then create the pairs themselves.\nTherefore, if we can avoid the creation of those large matrices, we can probably improve the speed.\n\n### R Based, Increment Count\n\n\n::: {.cell}\n\n```{.r .cell-code}\niterators_ici = function(x, y)\n{\n  n_entry = length(x)\n  sum_concordant = 0\n  sum_discordant = 0\n  for (i in seq(1, n_entry - 1)) {\n    for (j in seq(i + 1, n_entry)) {\n      sum_concordant = sum_concordant +  ((sign(x[i] - x[j]) * sign(y[i] - y[j])) > 0)\n      sum_discordant = sum_discordant + ((sign(x[i] - x[j]) * sign(y[i] - y[j])) < 0)\n    }\n  }\n  k_numerator = sum_concordant - sum_discordant\n  \n  x_is_dup = duplicated(x)\n  x_dup = x[x_is_dup]\n  x_tied_values_t1 = table(x_dup) + 1;\n  y_is_dup = duplicated(y)\n  y_dup = y[y_is_dup]\n  y_tied_values_t2 = table(y_dup) + 1\n  \n  x_tied_sum_t1 = sum(x_tied_values_t1 * (x_tied_values_t1 - 1)) / 2\n  y_tied_sum_t2 = sum(y_tied_values_t2 * (y_tied_values_t2 - 1)) / 2\n  t_0 = n_entry * (n_entry - 1) / 2\n  \n  k_denominator = sqrt((t_0 - x_tied_sum_t1) * (t_0 - y_tied_sum_t2))\n  k_numerator = sum_concordant - sum_discordant\n  \n  k_tau = k_numerator / k_denominator\n  \n  k_tau\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmicrobenchmark::microbenchmark(cor(x, y, method = \"kendall\"),\n                               reference_ici(x, y),\n                               iterators_ici(x, y),\n                               times = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: milliseconds\n                          expr       min        lq      mean    median\n cor(x, y, method = \"kendall\")  11.55812  12.49837  12.83441  12.69495\n           reference_ici(x, y) 357.29120 371.24522 387.07048 378.65192\n           iterators_ici(x, y) 279.28479 283.04712 289.51380 284.79317\n        uq       max neval\n  13.19846  14.45556    20\n 403.50524 438.96896    20\n 290.43204 335.47143    20\n```\n:::\n:::\n\n\nAllright!\nWe've got some decent improvement over our base attempt.\nWe also know, thanks to the SciPy project, that there is a better way to do get the number of concordant and discordant pairs (which is what we use in `{ICIKendallTau}`).\nLet's see if we can manage that in pure R, and see if it helps us out any.\n\n### R Based, Sort\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsort_ici = function(x, y)\n{\n\n  get_sort = function(values)\n  {\n    value_order = order(values, method = \"radix\")\n  }\n  \n  compare_self = function(x){\n    n_entry = length(x)\n    match_self = vector(\"integer\", n_entry)\n    match_self[1] = 1L\n  \n    idx = 2\n  \n    for (i in seq(2, n_entry)) {\n      if (x[i] != x[(i - 1)]) {\n        match_self[idx] = 1L;\n      } else {\n        match_self[idx] = 0L;\n      }\n      idx = idx + 1\n    }\n    return(match_self)\n  }\n  \n  compare_both = function(x, y)\n  {\n    n_entry = length(x)\n    match_self = vector(\"integer\", n_entry + 1)\n    match_self[1] = 1L\n    \n    idx = 2\n    \n    for (i in seq(2, n_entry)) {\n      if ((x[i] != x[(i - 1)]) || (y[i] != y[(i - 1)])) {\n        match_self[idx] = 1L\n      } else {\n        match_self[idx] = 0L\n      }\n      idx = idx + 1\n    }\n    match_self[n_entry + 1] = 1\n    return(match_self)\n  }\n\n  count_rank_tie = function(ranks)\n  {\n  \n    dup_ranks = duplicated(ranks)\n    ranks2 = ranks[dup_ranks]\n    number_tied = table(ranks2) + 1\n    \n    return(list(ntie = sum(number_tied * (number_tied - 1)) / 2,\n                t0 = sum(number_tied * (number_tied - 1) * (number_tied - 2)) / 2,\n                t1 = sum(number_tied * (number_tied - 1) * (2 * number_tied + 5))))\n  }\n  \n  which_notzero = function(x){\n    notzero = vector(\"integer\", length(x))\n    idx = 1L\n    \n    for (i in seq(1, length(x))) {\n      if (x[i] != 0) {\n        notzero[idx] = i - 1\n        idx = idx + 1L\n      }\n    }\n    keep_loc = seq(1, idx - 1)\n    notzero = notzero[keep_loc]\n    return(notzero)\n  }\n  \n  kendall_discordant = function(x, y){\n    #x = x4\n    #y = y4\n    sup = 1 + max(y)\n    \n    arr = vector(\"integer\", sup)\n    i = 0\n    k = 0\n    n = length(x)\n    idx = 1L\n    dis = 0\n    \n    while (i < n) {\n      while ((k < n) && (x[i + 1] == x[k + 1])) {\n        dis = dis + i\n        idx = y[k + 1]\n        while (idx != 0) {\n          dis = dis - arr[idx + 1]\n          idx = bitwAnd(idx, idx - 1)\n        }\n        k = k + 1\n      }\n      while (i < k) {\n        idx = y[i + 1]\n        while (idx < sup) {\n          arr[idx + 1] = arr[idx + 1] + 1\n          idx = idx + bitwAnd(idx, (-1*idx))\n        }\n        i = i + 1\n      }\n    }\n    dis\n  }\n\n\n  n_entry = length(x)\n  perm_y = get_sort(y)\n  x = x[perm_y]\n  y = y[perm_y]\n  y3 = compare_self(y)\n  y4 = cumsum(y3)\n  \n  perm_x = get_sort(x);\n  x = x[perm_x]\n  y4 = y4[perm_x]\n  x3 = compare_self(x)\n  x4 = cumsum(x3)\n  \n  obs = compare_both(x4, y4)\n  sum_obs = sum(obs)\n  cnt = diff(which_notzero(obs))\n  dis = kendall_discordant(x4, y4)\n  \n  ntie = sum((cnt * (cnt - 1)) / 2)\n  x_counts = count_rank_tie(x4)\n  xtie = x_counts[[1]]\n  x0 = x_counts[[2]]\n  x1 = x_counts[[3]]\n  \n  y_counts = count_rank_tie(y4)\n  ytie = y_counts[[1]]\n  y0 = y_counts[[2]]\n  y1 = y_counts[[3]]\n  \n  tot = (n_entry * (n_entry - 1)) / 2\n  con_minus_dis = tot - xtie - ytie + ntie - 2 * dis\n  tau = con_minus_dis / sqrt((tot - xtie) * (tot - ytie))\n  return(tau)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmicrobenchmark::microbenchmark(cor(x, y, method = \"kendall\"),\n                               reference_ici(x, y),\n                               iterators_ici(x, y),\n                               sort_ici(x, y),\n                               times = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: milliseconds\n                          expr        min        lq      mean     median\n cor(x, y, method = \"kendall\")  11.709530  12.33871  12.63373  12.488732\n           reference_ici(x, y) 355.224637 367.46692 387.70370 374.809739\n           iterators_ici(x, y) 273.673038 276.02535 280.80801 281.919287\n                sort_ici(x, y)   8.760096   8.98937  13.48845   9.395173\n        uq       max neval\n  12.87524  14.06242    20\n 421.60254 441.29727    20\n 283.32639 289.62743    20\n  10.02100  87.38422    20\n```\n:::\n:::\n\n\nSo, I thought I could implement the sorting method in R, and it would help.\nIt helps, in that it looks like it gets back to C speed, in R.\nWhich isn't bad, but still isn't great.\nThis is more than likely because I don't actually understand the sort based Kendall-tau algorithm on a theoretical level.\nI was able to copy it from the Python into `{Rcpp}`, and use a lot of debugging and test cases to make sure I had it implemented correctly, but there are things it is doing that I don't understand algorithmically, which means I don't know the best way to create an R-centric method for them.\nFor this example, I literally just translated my previous `{Rcpp}` code into R, which of course doesn't necessarily make for fast code.\n\n### C++ Based, Differences\n\nAs a reference, I also implemented the iterating differences algorithm in `{Rcpp}`.\nNote that this is not meant to be called by normal users of `{ICIKendallTau}`, we have it there as a reference to make sure that everything else is working properly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmicrobenchmark::microbenchmark(cor(x, y, method = \"kendall\"),\n                               reference_ici(x, y),\n                               iterators_ici(x, y),\n                               sort_ici(x, y),\n                               ICIKendallTau:::ici_kt_pairs(x, y),\n                               times = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: milliseconds\n                               expr        min         lq       mean     median\n      cor(x, y, method = \"kendall\")  11.829116  12.051790  12.872894  12.815848\n                reference_ici(x, y) 347.066153 382.616886 400.650147 395.033647\n                iterators_ici(x, y) 271.020774 273.111584 284.668516 274.988955\n                     sort_ici(x, y)   8.978789   9.393873  10.498460  10.161402\n ICIKendallTau:::ici_kt_pairs(x, y)   4.336266   4.493440   4.806514   4.771994\n         uq        max neval\n  13.380157  14.977161    20\n 418.945940 465.485468    20\n 292.175271 342.778833    20\n  11.554007  13.605138    20\n   4.891025   6.397218    20\n```\n:::\n:::\n\n\nWe can see that is faster than the C-based `cor` by maybe 2X for 1000 long vectors.\nGiven that `ici_kt_pairs` has only the single iterator logic for the one correlation method, that makes sense.\n\n### C++ Based, Sort\n\nLet's compare the actual sort-based function that is implemented in the `{ICIKendallTau}` package (which you can see in [@iciktcode]).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmicrobenchmark::microbenchmark(cor(x, y, method = \"kendall\"),\n                               reference_ici(x, y),\n                               iterators_ici(x, y),\n                               sort_ici(x, y),\n                               ici_kt(x, y),\n                               times = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: microseconds\n                          expr        min          lq        mean     median\n cor(x, y, method = \"kendall\")  11922.472  12154.2850  12992.5719  12916.488\n           reference_ici(x, y) 352612.664 380621.5915 399187.5410 393977.078\n           iterators_ici(x, y) 266384.200 270873.1345 286840.9181 273214.937\n                sort_ici(x, y)   8862.455   9607.5720  10356.4879  10301.072\n                  ici_kt(x, y)    216.761    245.6465    273.3186    265.784\n         uq        max neval\n  13522.521  16085.135    20\n 424152.088 444856.163    20\n 289494.598 338773.242    20\n  11140.300  12458.339    20\n    291.693    367.443    20\n```\n:::\n:::\n\n\nThat's fast!\nNotice the time moved from *milliseconds* to *microseconds*, and the `{ICIKendallTau}` version is ~ 50X faster than the base R version.\nLet's increase the size of our vectors by 10X and compare the run times again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_large = rnorm(10000)\ny_large = rnorm(10000)\nmicrobenchmark::microbenchmark(cor(x, y, method = \"kendall\"),\n                               cor(x_large, y_large, method = \"kendall\"),\n                               ici_kt(x, y),\n                               ici_kt(x_large, y_large),\n                               times = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: microseconds\n                                      expr         min           lq\n             cor(x, y, method = \"kendall\")   11654.425   11757.9275\n cor(x_large, y_large, method = \"kendall\") 1134471.513 1149212.6425\n                              ici_kt(x, y)     211.174     228.8565\n                  ici_kt(x_large, y_large)    2341.995    2370.5770\n         mean      median          uq         max neval\n   12088.3777   11816.745   12018.914   15825.794    20\n 1201616.7482 1168728.874 1243587.177 1456530.477    20\n     259.2706     247.703     265.988     531.182    20\n    2643.4572    2392.539    2453.216    4024.673    20\n```\n:::\n:::\n\n\nThe base R method increases time taken by 100X, but the sort based method of `{ICIKendallTau}` increases only 10X.\nThis is the advantage of the sort method, it's complexity is $\\mathcal{O}(n\\log{}n))$, *vs* $\\mathcal{O}(n^2)$.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}