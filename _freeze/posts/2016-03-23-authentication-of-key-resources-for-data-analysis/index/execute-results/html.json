{
  "hash": "4e7aa45ce848419fc61d2934b2aec674",
  "result": {
    "markdown": "---\ntitle: \"Authentication of Key Resources for Data Analysis\"\ndate: 2016-03-23\ncategories: [reproducibility, open-science, analysis]\nsubtitle: |\n  NIH is asking for authentication of key resources. How does this apply to data analyses?\n---\n\n\n## TL;DR\n\nNIH recently introduced a reproducibility initiative, extending to including the \"Authentication of Key Resources\" page in grant applications from Jan 25, 2016.\nSeems to be intended for grants involving biological reagents, but we included it in our recent R03 grant developing new data analysis methods. \nWe believe that this type of thing should become common for all grants, not just those that use biological/chemical resources.\n\n## NIH and Reproducibility\n\nThere has been a lot of things published recently about the *reproducibility \ncrisis* in science (see refs). The federal funding agencies are starting to \nrespond to this, and beginning with grants submitted after January 25, 2016, \ngrants are [supposed to address the\nreproducibility](http://grants.nih.gov/reproducibility/index.htm) of the work\nproposed, including the presence of various confounding factors (i.e. sex of\nanimals, the source of cell lines, etc). In addition to this, there is a new\ndocument that can be added to grants, the [**Authentication\nPlan**](http://nexus.od.nih.gov/all/2016/01/29/authentication-of-key-biological-andor-chemical-resources-in-nih-grant-applications/),\nwhich as far as I can tell is intended specifically for:\n\n> key biological and/or chemical resources used in the proposed studies\n\nNow, this makes sense. Some sources of irreproducibility include, but are not \nlimited to:\n\n  * unvalidated antibodies \n  * cell lines that are not what was thought \n  * impure chemicals\n\nI think this is a **good thing**. What does it have to do with data analysis?\n\n## Data / Code Authentication\n\nWhen we were submitting a recent R03 proposal for developing novel data analysis\nmethods and statistical tools, the grant management office asked us about the \n**Authentication of Key Resources** attachment, which we completely missed. Upon\nreview of the guidelines, we initially determined that this document did not\napply. However, we decided to go ahead and take some initiative.\n\n### Data Authentication?\n\nWhen dealing with multiple samples from high-throughput samples, there are\nfrequently a few easy ways to examine the data quality, and although it can be\nhard to verify that the data **is what the supplier says it is**, which would be\ntrue **authentication**, there are some ways to verify that the various samples\nin the dataset are at least self-consistent within each sample class (normal and\ndisease, condition 1 and condition 2).\n\nMy go-to for data self-consistency are principal components analysis (PCA) and \ncorrelation heat-maps. Correlation heat-maps involve calculating \nall of the pairwise sample to sample correlations using all of the non-zero\nsample features (those that are non-zero in the two pairs being compared). These\nheatmaps, combined with the sample class information, and clustering within each\nclass, are a nice visual way to eyeball samples that have potential problems. A\nsimple example for RNA-seq transcriptomics was shown in [Gierli≈Ñski et al., \nStatistical models for RNA-seq data derived from a two-condition 48-replicate\nexperiment Bioinformatics (2015) 31 (22): 3625-3630, Figure\n1](https://dx.doi.org/10.1093/bioinformatics/btv425).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::include_graphics(\"yeast_48rep_cor_heatmap.jpg\")\n```\n\n::: {.cell-output-display}\n![](yeast_48rep_cor_heatmap.jpg)\n:::\n:::\n\n\nThe other measures they used in this paper are also very nice, in plotting the \nmedian correlation of a sample against all other samples, and the fraction of \noutlier features in a given sample (see figure 2 of Gierlinkski et al). The\nfinal measure they propose is not generally applicable to all -omics data\nhowever.\n\nPCA on the data, followed by visualizing the scores on the first few principal \ncomponents, and colored by sample class (or experimental condition) is similar \nin spirit to the correlation heat-map. In fact, it is very similar, because PCA \nis actually decomposing on the covariance of the samples, which is very related\nto the correlations (an early algorithm actually used the correlation matrix).\n\nBoth of these methods can highlight possible problems with individual samples,\nand make sure that the set of data going into the analysis is at least\nself-consistent, which is important when doing classification or differential\nabundance analyses.\n\n### Code Authentication\n\nThe other thing we highlighted in the document was **code** authentication. In \nthis case, we highlighted the use of unit-testing in the R packages that we are \nplanning to develop. Even though this is software coming out of a research lab,\nwe need to have confidence that the functions we write return the correct values\ngiven various inputs. In addition, code testing coverage helps evaluate that we\nare testing *all* of the functionality by checking that all of the lines in our\ncode are run by the tests. Finally, we are also planning to write tests for core\nfunctions provided by others (i.e. functions in other R packages), in that they\nwork as we expect, by returning correct values given specific inputs.\n\n## Conclusion\n\nGoing forward, I think it would be a good thing if people writing research grants\nfor data analysis methods would discuss how they are going to look at the data\nto assess it's quality, and how they are going to do unit testing, and will have\nto start saying that they are going to do unit testing of their analysis method.\n\nI'd be interested in others' thoughts on this as well.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}