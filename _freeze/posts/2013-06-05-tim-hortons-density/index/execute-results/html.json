{
  "hash": "098aa04dae2b2fcc51ae35a9a6f8c073",
  "result": {
    "markdown": "---\ntitle: \"Tim Hortons Density\"\ndate: 2013-06-05\ncategories: [R, mapping, tim-hortons]\nsubtitle: |\n  How far away are most Canadians from a Tim Hortons?\n---\n\n\nInspired by this [post](http://www.ifweassume.com/2012/10/the-united-states-of-starbucks.html), I wanted to examine the locations and density of Tim Hortons restaurants in Canada. Using Stats Canada data, each census tract is queried on Foursquare for Tims locations.\n\n## Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(stringsAsFactors=F)\nrequire(timmysDensity)\nrequire(plyr)\nrequire(maps)\nrequire(ggplot2)\nrequire(geosphere)\n```\n:::\n\n\n## Statistics Canada Census Data\n\nThe actual Statistics Canada data at the dissemination block level can be downloaded from [here](http://www.data.gc.ca/default.asp?lang=En&n=5175A6F0-1&xsl=datacataloguerecord&metaxsl=datacataloguerecord&formid=C87D5FDD-00E6-41A0-B5BA-E8E41B521ED0). You will want to download the Excel format, read it, and then save it as either tab-delimited or CSV using a non-standard delimiter, I used a semi-colon (;).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensusData <- read.table(\"../timmysData/2011_92-151_XBB_XLSX.csv\", header=F, sep=\";\", quote=\"\")\ncensusData <- censusData[,1:17]\nnames(censusData) <- c(\"DBuid\", \"DBpop2011\", \"DBtdwell2011\", \"DBurdwell2011\", \"DBarea\", \"DB_ir2011\", \"DAuid\", \"DAlamx\", \"DAlamy\", \"DAlat\",\n                       \"DAlong\", \"PRuid\", \"PRname\", \"PRename\", \"PRfname\", \"PReabbr\", \"PRfabbr\")\ncensusData$DBpop2011 <- as.numeric(censusData$DBpop2011)\ncensusData$DBpop2011[is.na(censusData$DBpop2011)] <- 0\n\ncensusData$DBtdwell2011 <- as.numeric(censusData$DBtdwell2011)\ncensusData$DBtdwell2011[is.na(censusData$DBtdwell2011)] <- 0\n```\n:::\n\n\nFrom this data we get block level:\n\n  * populations (DBpop2011)\n  * total private dwellings (DBtdwell2011)\n  * privale dwellings occupied by usual residents (DBurdwell2011)\n  * block land area (DBarea)\n  * dissemination area id (DAuid)\n  * representative point x coordinate in Lambert projection (DAlamx)\n  * rep. point y coordinate in Lambert projection (DAlamy)\n  * rep. point latitude (DAlat)\n  * rep. point longitude (DAlong)\n  \nThis should be everything we need to do the investigation we want.\n\n## Dissemination Area Long. and Lat.\n\nWe need to find the unique dissemination areas, and get out their latitudes and longitudes for querying in other databases. Note that the longitude and latitude provided here actually are weighted representative locations based on population. However, given the size of them, I don't think using them will be a problem for `Foursquare`. Because areas are what we have location data for, we will summarize everything at the area level, summing the population counts for all the blocks within an area.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuniqAreas <- unique(censusData$DAuid)\n\nsummarizeArea <- function(areaID){\n  areaData <- censusData[(censusData$DAuid == areaID),]\n  outData <- data.frame(uid=areaID, lamx=areaData[1,\"DAlamx\"], lamy=areaData[1,\"DAlamy\"], lat=areaData[1,\"DAlat\"], long=areaData[1,\"DAlong\"], pop=sum(areaData[,\"DBpop2011\"]), dwell=sum(areaData[,\"DBtdwell2011\"]), prov=areaData[1, \"PRename\"])\n  return(outData)\n}\nareaData <- adply(uniqAreas, 1, summarizeArea)\n.sessionInfo <- sessionInfo()\n.timedate <- Sys.time()\nwrite.table(areaData, file=\"../timmysData/areaData.txt\", sep=\"\\t\", row.names=F, col.names=T)\nsave(areaData, .sessionInfo, .timedate, file=\"../timmysData/areaDataFile.RData\", compress=\"xz\")\n```\n:::\n\n\n## Run queries on Foursquare\n\n### Load up the data and verify what we have.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"../timmysData/areaDataFile.RData\")\nhead(areaData)\n```\n:::\n\n\n\n### Generate queries and run\n\nFor each dissemination area (DA), we are going to use as the location for the query the latitude and longitude of each DA, as well as the search string \"tim horton\". \n\nBecause Foursquare limits the number of userless requests to [5000 / hr](https://developer.foursquare.com/overview/ratelimits). To make sure we stay under this limit, the `runQueries` function will only 5000 queries an hour.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrunQueries(areaData, idFile=\"../timmysData/clientid.txt\", secretFile=\"../timmysData/clientsecret.txt\", outFile=\"../timmysData/timmysLocs2.txt\")\n```\n:::\n\n\n\n### Clean up the results\n\nDue to the small size of the DAs, we have a lot of duplicate entries. Now lets remove all the duplicate entries.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncleanUpResults(\"../timmysData/timmysLocs2.txt\")\n```\n:::\n\n\n\n## Visualize Locations\n\nFirst lets read in the data and make sure that we have Tims locations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read in and clean up the data\ntimsLocs <- scan(file=\"../timmysData/timmysLocs2.txt\", what=character(), sep=\"\\n\")\ntimsLocs <- strsplit(timsLocs, \":\")\n\ntimsName <- sapply(timsLocs, function(x){x[1]})\ntimsLat <- sapply(timsLocs, function(x){x[2]})\ntimsLong <- sapply(timsLocs, function(x){x[3]})\n\nlocData <- data.frame(description=timsName, lat=as.numeric(timsLat), long=as.numeric(timsLong))\nhasNA <- is.na(locData[,\"lat\"]) | is.na(locData[,\"long\"])\nlocData <- locData[!(hasNA),]\n\ntimsStr <- c(\"tim hortons\", \"tim horton's\")\n\nhasTims <- (grepl(timsStr[1], locData$description, ignore.case=T)) | (grepl(timsStr[2], locData$description, ignore.case=T))\n\nlocData <- locData[hasTims,]\ntimsLocs <- locData\nrm(timsName, timsLat, timsLong, hasNA, locData, hasTims, timsStr)\n.timedate <- Sys.time()\n.sessionInfo <- sessionInfo()\nsave(timsLocs, .timedate, .sessionInfo, file=\"../timmysData/timsLocs.RData\", compress=\"xz\")\n```\n:::\n\n\n### Put them on a map\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(timsLocs)\ndata(areaDataFile)\ncanada <- map_data(\"world\", \"canada\")\n\np <- ggplot(legend=FALSE) +\n  geom_polygon( data=canada, aes(x=long, y=lat,group=group)) +\n  theme(panel.background = element_blank()) +\n  theme(panel.grid.major = element_blank()) +\n  theme(panel.grid.minor = element_blank()) +\n  theme(axis.text.x = element_blank(),axis.text.y = element_blank()) +\n  theme(axis.ticks = element_blank()) +\n  xlab(\"\") + ylab(\"\")\n\nsp <- timsLocs[1, c(\"lat\", \"long\")]\n\np2 <- p + geom_point(data=timsLocs[,c(\"lat\", \"long\")], aes(x=long, y=lat), colour=\"green\", size=1, alpha=0.5)\n\nprint(p2)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](mapIt-1.png){width=672}\n:::\n:::\n\n\n### How far??\n\nAnd now lets also calculate the minimum distance of a given DA from Timmys locations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqueryLocs <- matrix(c(timsLocs$long, timsLocs$lat), nrow=nrow(timsLocs), ncol=2, byrow=F) # these are the tims locations\ndistLocs <- matrix(c(areaData$long, areaData$lat), nrow=nrow(areaData), ncol=2, byrow=F) # the census centers\nallDists <- apply(queryLocs, 1, function(x){\n  min(distHaversine(x, distLocs)) # only need the minimum value to determine \n})\n```\n:::\n\n\n\nFrom the `allDists` variable above, we can determine that the maximum distance any census dissemination area (DA) is from a Tim Hortons is 51.5 km (31.9815 miles). This is based on distances calculated \"as the crow flies\", but still, that is pretty close. Assuming roads, the furthest a Canadian should have to travel is less than an hour to get their Timmys fix. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntotPopulation <- sum(areaData$pop, na.rm=T)\nlessDist <- seq(50, 51.6 * 1000, 50) # distances are in meters, so multiply by 1000 to get reasonable km\n\npercPop <- sapply(lessDist, function(inDist){\n  isLess <- allDists < inDist\n  sum(areaData$pop[isLess], na.rm=T) / totPopulation * 100\n})\n\nplotDistPerc <- data.frame(distance=lessDist, population=percPop, logDist=log10(lessDist))\nggplot(plotDistPerc, aes(x=logDist, y=population)) + geom_point() + xlab(\"Log10 Distance\") + ylab(\"% Population\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](percPopulation-1.png){width=672}\n:::\n:::\n\n\n\nWhat gets really interesting, is how much of the population lives within a given distance of a Timmys. By summing up the percentage of the population within given distances. The plot above shows that 50% of the population is within 316.227766 **meters** of a Tim Hortons location. \n\nI guess Canadians really do like their Tim Hortons Coffee (and donuts!).\n\n## Replication\n\nAll of the necessary processed data and code is available in the `R` package [`timmysDensity`](https://github.com/rmflight/timmysDensity). You can install it using `devtools`. The original data files are linked in the relevant sections above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(devtools)\ninstall_github('timmysDensity', 'rmflight')\n```\n:::\n\n\n### Caveats\n\nI originally did this work based on a different set of data, that I have not been able to locate the original source for. I have not compared these results to that data to verify their accuracy. When I do so, I will update the package, vignette and blog post.\n\n## Posted\n\nThis work exists as the vignette of [`timmysDensity`](https://github.com/rmflight/timmysDensity), on my [web-blog](https://rmflight.github.io/posts/2013/06/timmysDensity.html), and independently as the front page for the [GitHub repo](http://rmflight.github.io/timmysDensity).\n\n## Disclaimer\n\nTim Hortons was not involved in the creation or preparation of this work. I am not regularly updating the location information obtained from Foursquare, it is only valid for May 31, 2013. All code used in preparing these results was written by me, except in the case where code from other `R` packages was used. All opinions and conclusions are my own, and do not reflect the views of anyone else or any institution I may be associated with.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}