{
  "hash": "f0beb6da4777d5d82fb5efe377fa5d58",
  "result": {
    "markdown": "---\ntitle: \"Random Forest Classification Using Parsnip\"\nsubtitle: |\n  How to make sure you get a classification fit and not a probability fit from a random forest model using the tidymodels framework.\ndate: 08-30-2021\ncategories: [parsnip, tidymodels, machine-learning, random-forest, random-code-snippets]\nbibliography: refs.bib\neditor_options: \n  chunk_output_type: console\n---\n\n\n\nI've been working on a \"machine learning\" project, and in the process I've been learning to use the tidymodels framework [@tidymodelsSite], which helps save you from leaking information from testing to training data, as well as creating workflows in a consistent way across methods.\n\nHowever, I got tripped up recently by one issue.\nWhen I've previously used Random Forests [@rfWiki], I've found that for **classification** problems, the out-of-bag (OOB) error reported is a good proxy for the area-under-the-curve (AUC), or estimate of how good any other machine learning technique will do (see [@flight2015random] for an example using actual random data).\nTherefore, I like to put my data through a Random Forest algorithm and check the OOB error, and then maybe reach for a tuned boosted tree to squeeze every last bit of performance out.\n\ntidymodels default is to use a `probability` tree, even for classification problems.\nThis isn't normally a problem for most people, because you will have a train and test set, and estimate performance on the test set using AUC.\nHowever, it is a problem if you just want to see the OOB error from the random forest, because it is reported differently for probability vs classification.\n\nLets run an example using the tidymodels *cell* data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\nlibrary(modeldata)\nlibrary(skimr)\ndata(cells, package = \"modeldata\")\nlibrary(ranger)\ntidymodels_prefer()\n\ncells$case = NULL\nset.seed(1234)\nranger(class ~ ., data = cells, min.node.size = 10, classification = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRanger result\n\nCall:\n ranger(class ~ ., data = cells, min.node.size = 10, classification = TRUE) \n\nType:                             Classification \nNumber of trees:                  500 \nSample size:                      2019 \nNumber of independent variables:  56 \nMtry:                             7 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error:             17.29 % \n```\n:::\n:::\n\n\nHere we can see that we get an OOB error of 17%, which isn't too shabby.\nNow, let's setup a workflow to do the same thing via tidymodels parsnip.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_spec = rand_forest() %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\nrf_recipe = recipe(class ~ ., data = cells) %>%\n  step_dummy(class, -class)\n\nset.seed(1234)\nworkflow() %>%\n  add_recipe(rf_recipe) %>%\n  add_model(rf_spec) %>%\n  fit(data = cells)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.threads = 1,      verbose = FALSE, seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      2019 \nNumber of independent variables:  56 \nMtry:                             7 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1198456 \n```\n:::\n:::\n\n\nHere we see the OOB error is 12% (0.119), which is not significantly different than the 17% above, but still different.\nAlso, the \"Type\" shows \"Probability estimation\" instead of \"Classification estimation\".\n\nIf we run ranger again with a \"probability\" instead of \"classification\", do we match up with the result above?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nranger(class ~ ., data = cells, min.node.size = 10, probability = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRanger result\n\nCall:\n ranger(class ~ ., data = cells, min.node.size = 10, probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      2019 \nNumber of independent variables:  56 \nMtry:                             7 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.119976 \n```\n:::\n:::\n\n\nThat is much closer to the tidymodels result!\nGreat!\nExcept, it's a misestimation of the true OOB error for classification.\nHow do we get what we want while using the tidymodels framework?\n\nI couldn't find the answer, and the above **looked** like a bug, so I filed one on the parsnip github [@parsnipIssue].\nJulia Silge helpfully provided the solution to my problem.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_spec_class = rand_forest() %>%\n  set_engine(\"ranger\", probability = FALSE) %>%\n  set_mode(\"classification\")\n\nset.seed(1234)\nworkflow() %>%\n  add_recipe(rf_recipe) %>%\n  add_model(rf_spec_class) %>%\n  fit(data = cells)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n1 Recipe Step\n\n• step_dummy()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, probability = ~FALSE,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1)) \n\nType:                             Classification \nNumber of trees:                  500 \nSample size:                      2019 \nNumber of independent variables:  56 \nMtry:                             7 \nTarget node size:                 1 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error:             16.54 % \n```\n:::\n:::\n\n\nAha!\nNow we are much closer to the original value of 17%, and the \"Type\" is \"Classification\".\n\nI know in this case, the differences in OOB error are honestly not that much different, but in my recent project, they differed by 20%, where I had a 45% using classification, and 25% using probability.\nTherefore, I was being fooled by the tidymodels framework investigation, and then wondering why my final AUC on a tuned model was only hitting just > 55%.\n\nSo remember, this isn't how I would run the model for final classification and estimation of AUC on a test set, but if you want the OOB errors for a quick \"feel\" of your data, it's very useful.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}