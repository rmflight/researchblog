{
  "hash": "527af45d15c5ca43b78d98ad4e4dfba4",
  "result": {
    "markdown": "---\ntitle: \"Random Forest vs PLS on Random Data\"\ndate: 2015-12-12\ncategories: [random-forest, machine-learning, partial-least-squares, statistics, analysis]\nsubtitle: |\n  Comparing random-forest and partial-least-squares discriminant-analysis on random data to show the problems inherent in PLS-DA.\n---\n\n\n## TL;DR\n\nPartial least squares (PLS) discriminant-analysis (DA) can ridiculously over fit\neven on completely random data. The quality of the PLS-DA model can be assessed\nusing cross-validation, but cross-validation is not typically performed in many\nmetabolomics publications. Random forest, in contrast, because of the *forest* of\ndecision tree learners, and the out-of-bag (OOB) samples used for testing each tree, \nautomatically provides an indication of the quality of the model.\n\n## Why?\n\nI've recently been working on some machine learning work using **random forests**\n(RF) [Breimann, 2001](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) on metabolomics data. This has been relatively successful,\nwith decent sensitivity and specificity, and hopefully I'll be able to post more\nsoon. However, PLS (Wold, 1975) is a standard technique used in metabolomics\ndue to the prevalence of analytical chemists in metabolomics and a long familiarity\nwith the method. Importantly, my collaborators frequently use PLS-DA to generate\nplots to show that the various classes of samples are separable.\n\nHowever, it has long been known that PLS (and all of it's variants, PLS-DA, OPLS,\nOPLS-DA, etc) can easily generate models that over fit the data, and that over fitting\nof the model needs to be assessed if the model is going to be used in subsequent\nanalyses. \n\n## Random Data\n\nTo illustrate the behavior of both RF and PLS-DA, we will generate some random data\nwhere each of the samples are randomly assigned to one of two classes.\n\n### Feature Intensities\n\nWe will generate a data set with 1000 features, where each feature's mean value\nis from a uniform distribution with a range of 1-10000.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\ntheme_set(cowplot::theme_cowplot())\nlibrary(fakeDataWithError)\nset.seed(1234)\nn_point <- 1000\nmax_value <- 10000\ninit_values <- runif(n_point, 0, max_value)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ninit_data <- data.frame(data = init_values)\nggplot(init_data, aes(x = data)) + geom_histogram() + ggtitle(\"Initial Data\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot_initial-1.png){width=672}\n:::\n:::\n\n\nFor each of these features, their distribution across samples will be based on\na random normal distribution where the mean is the initial feature value and a\nstandard deviation of 200. The number of samples is 100.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_sample <- 100\nerror_values <- add_uniform_noise(n_sample, init_values, 200)\n```\n:::\n\n\nJust for information, the `add_uniform_noise` function is this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadd_uniform_noise\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfunction (n_rep, value, sd, use_zero = FALSE) \n{\n    n_value <- length(value)\n    n_sd <- n_rep * n_value\n    out_sd <- rnorm(n_sd, 0, sd)\n    out_sd <- matrix(out_sd, nrow = n_value, ncol = n_rep)\n    if (!use_zero) {\n        tmp_value <- matrix(value, nrow = n_value, ncol = n_rep, \n            byrow = FALSE)\n        out_value <- tmp_value + out_sd\n    }\n    else {\n        out_value <- out_sd\n    }\n    return(out_value)\n}\n<bytecode: 0x557b6812fbf8>\n<environment: namespace:fakeDataWithError>\n```\n:::\n:::\n\n\nI created it as part of a package that is able to add different kinds of noise\nto data.\n\nThe distribution of values for a single feature looks like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nerror_data <- data.frame(feature_1 = error_values[1,])\nggplot(error_data, aes(x = feature_1)) + geom_histogram() + ggtitle(\"Error Data\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot_error-1.png){width=672}\n:::\n:::\n\n\nAnd we will assign the first 50 samples to **class_1** and the second 50 samples\nto **class_2**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_class <- rep(c(\"class_1\", \"class_2\"), each = 50)\nsample_class\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n  [8] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [15] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [22] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [29] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [36] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [43] \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\" \"class_1\"\n [50] \"class_1\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [57] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [64] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [71] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [78] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [85] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [92] \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\" \"class_2\"\n [99] \"class_2\" \"class_2\"\n```\n:::\n:::\n\n\n## PCA\n\nJust to show that the data is pretty random, lets use principal components\nanalysis (PCA) to do a decomposition, and plot the first two components:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_pca <- prcomp(t(error_values), center = TRUE, scale. = TRUE)\npca_data <- as.data.frame(tmp_pca$x[, 1:2])\npca_data$class <- as.factor(sample_class)\nggplot(pca_data, aes(x = PC1, y = PC2, color = class)) + geom_point(size = 4)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/pca_data-1.png){width=672}\n:::\n:::\n\n\n\n## Random Forest\n\nLet's use RF first, and see how things look.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrandomForest 4.7-1.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nType rfNews() to see new features/changes/bug fixes.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'randomForest'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    margin\n```\n:::\n\n```{.r .cell-code}\nrf_model <- randomForest(t(error_values), y = as.factor(sample_class))\n```\n:::\n\n\nThe confusion matrix comparing actual *vs* predicted classes based on the \nout of bag (OOB) samples:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::kable(rf_model$confusion)\n```\n\n::: {.cell-output-display}\n|        | class_1| class_2| class.error|\n|:-------|-------:|-------:|-----------:|\n|class_1 |      21|      29|        0.58|\n|class_2 |      28|      22|        0.56|\n:::\n:::\n\n\nAnd an overall error of 0.5760364.\n\n## PLS-DA\n\nSo PLS-DA is really just PLS with **y** variable that is binary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n```\n:::\n\n```{.r .cell-code}\npls_model <- plsda(t(error_values), as.factor(sample_class), ncomp = 2)\npls_scores <- data.frame(comp1 = pls_model$scores[,1], comp2 = pls_model$scores[,2], class = sample_class)\n```\n:::\n\n\nAnd plot the PLS scores:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(pls_scores, aes(x = comp1, y = comp2, color = class)) + geom_point(size = 4) + ggtitle(\"PLS-DA of Random Data\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot_plsda-1.png){width=672}\n:::\n:::\n\n\nAnd voila! Perfectly separated data! If I didn't tell you that it was random, would\nyou suspect it?\n\n## Cross-validated PLS-DA\n\nOf course, one way to truly assess the worth of the model would be to use\ncross-validation, where a fraction of data is held back, and the model trained\non the rest. Predictions are then made on the held back fraction, and because we\nknow the truth, we will then calculate the **area under the reciever operator curve**\n(AUROC) or area under the curve (AUC) created by plotting true positives *vs* \nfalse positives.\n\nTo do this we will need two functions:\n\n1. Generates all of the CV folds\n2. Generates PLS-DA model, does prediction on hold out, calculates AUC\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cvTools)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: robustbase\n```\n:::\n\n```{.r .cell-code}\nlibrary(ROCR)\n\ngen_cv <- function(xdata, ydata, nrep, kfold){\n  n_sample <- length(ydata)\n  all_index <- seq(1, n_sample)\n  cv_data <- cvFolds(n_sample, K = kfold, R = nrep, type = \"random\")\n  \n  rep_values <- vapply(seq(1, nrep), function(in_rep){\n    use_rep <- cv_data$subsets[, in_rep]\n    cv_values <- vapply(seq(1, kfold), function(in_fold){\n      test_index <- use_rep[cv_data$which == in_fold]\n      train_index <- all_index[-test_index]\n      \n      plsda_cv(xdata[train_index, ], ydata[train_index], xdata[test_index, ],\n               ydata[test_index])\n    }, numeric(1))\n  }, numeric(kfold))\n}\n\nplsda_cv <- function(xtrain, ytrain, xtest, ytest){\n  pls_model <- plsda(xtrain, ytrain, ncomp = 2)\n  pls_pred <- predict(pls_model, xtest, type = \"prob\")\n  \n  use_pred <- pls_pred[, 2, 1]\n  \n  pred_perf <- ROCR::prediction(use_pred, ytest)\n  pred_auc <- ROCR::performance(pred_perf, \"auc\")@y.values[[1]]\n  return(pred_auc)\n}\n```\n:::\n\n\nAnd now lets do a bunch of replicates (100).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_vals <- gen_cv(t(error_values), factor(sample_class), nrep = 100, kfold = 5)\n\nmean(cv_vals)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4260387\n```\n:::\n\n```{.r .cell-code}\nsd(cv_vals)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1188491\n```\n:::\n\n```{.r .cell-code}\ncv_frame <- data.frame(auc = as.vector(cv_vals))\nggplot(cv_frame, aes(x = auc)) + geom_histogram(binwidth = 0.01)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/rep_plsda-1.png){width=672}\n:::\n:::\n\n\nSo we get an average AUC of 0.4260387, which is pretty awful. This implies\nthat even though there was good separation on the scores, maybe the model is\nnot actually that good, and we should be cautious of any predictions being made.\n\nOf course, the PCA at the beginning of the analysis shows that there is no *real*\nseparation in the data in the first place. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndevtools::session_info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23)\n os       Pop!_OS 22.04 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_US:en\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2022-12-02\n pandoc   2.19.2 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package           * version    date (UTC) lib source\n   assertthat          0.2.1      2019-03-21 [2] CRAN (R 4.2.1)\n P cachem              1.0.6      2021-08-19 [?] CRAN (R 4.2.1)\n   callr               3.7.2      2022-08-22 [2] CRAN (R 4.2.1)\n P caret             * 6.0-93     2022-08-09 [?] CRAN (R 4.2.1)\n P class               7.3-20     2022-01-13 [?] CRAN (R 4.2.1)\n   cli                 3.4.0      2022-09-08 [2] CRAN (R 4.2.1)\n   codetools           0.2-18     2020-11-04 [2] CRAN (R 4.2.1)\n   colorspace          2.0-3      2022-02-21 [2] CRAN (R 4.2.1)\n   cowplot             1.1.1      2020-12-30 [2] CRAN (R 4.2.1)\n   crayon              1.5.1      2022-03-26 [2] CRAN (R 4.2.1)\n P cvTools           * 0.3.2      2012-05-14 [?] CRAN (R 4.2.1)\n   data.table          1.14.2     2021-09-27 [2] CRAN (R 4.2.1)\n   DBI                 1.1.3      2022-06-18 [2] CRAN (R 4.2.1)\n   DEoptimR            1.0-11     2022-04-03 [2] CRAN (R 4.2.1)\n   devtools            2.4.4      2022-07-20 [2] CRAN (R 4.2.1)\n P digest              0.6.29     2021-12-01 [?] CRAN (R 4.2.1)\n   dplyr               1.0.10     2022-09-01 [2] CRAN (R 4.2.1)\n   ellipsis            0.3.2      2021-04-29 [2] CRAN (R 4.2.1)\n P evaluate            0.16       2022-08-09 [?] CRAN (R 4.2.1)\n   fakeDataWithError * 0.0.1      2022-12-02 [1] Github (rmflight/fakeDataWithError@ccd8714)\n   fansi               1.0.3      2022-03-24 [2] CRAN (R 4.2.1)\n   farver              2.1.1      2022-07-06 [2] CRAN (R 4.2.1)\n P fastmap             1.1.0      2021-01-25 [?] CRAN (R 4.2.1)\n   foreach             1.5.2      2022-02-02 [2] CRAN (R 4.2.1)\n P fs                  1.5.2      2021-12-08 [?] CRAN (R 4.2.1)\n   future              1.28.0     2022-09-02 [2] CRAN (R 4.2.1)\n P future.apply        1.10.0     2022-11-05 [?] CRAN (R 4.2.1)\n   generics            0.1.3      2022-07-05 [2] CRAN (R 4.2.1)\n P ggplot2           * 3.4.0      2022-11-04 [?] CRAN (R 4.2.1)\n   globals             0.16.1     2022-08-28 [2] CRAN (R 4.2.1)\n P glue                1.6.2      2022-02-24 [?] CRAN (R 4.2.1)\n P gower               1.0.0      2022-02-03 [?] CRAN (R 4.2.1)\n   gtable              0.3.1      2022-09-01 [2] CRAN (R 4.2.1)\n P hardhat             1.2.0      2022-06-30 [?] CRAN (R 4.2.1)\n P highr               0.9        2021-04-16 [?] CRAN (R 4.2.1)\n P htmltools           0.5.3      2022-07-18 [?] CRAN (R 4.2.1)\n   htmlwidgets         1.5.4      2021-09-08 [2] CRAN (R 4.2.1)\n   httpuv              1.6.6      2022-09-08 [2] CRAN (R 4.2.1)\n P ipred               0.9-13     2022-06-02 [?] CRAN (R 4.2.1)\n   iterators           1.0.14     2022-02-05 [2] CRAN (R 4.2.1)\n P jsonlite            1.8.0      2022-02-22 [?] CRAN (R 4.2.1)\n P knitr               1.40       2022-08-24 [?] CRAN (R 4.2.1)\n   labeling            0.4.2      2020-10-20 [2] CRAN (R 4.2.1)\n   later               1.3.0      2021-08-18 [2] CRAN (R 4.2.1)\n   lattice           * 0.20-45    2021-09-22 [2] CRAN (R 4.2.1)\n P lava                1.7.0      2022-10-25 [?] CRAN (R 4.2.1)\n P lifecycle           1.0.3      2022-10-07 [?] CRAN (R 4.2.1)\n   listenv             0.8.0      2019-12-05 [2] CRAN (R 4.2.1)\n   lubridate           1.8.0      2021-10-07 [2] CRAN (R 4.2.1)\n P magrittr            2.0.3      2022-03-30 [?] CRAN (R 4.2.1)\n   MASS                7.3-58.1   2022-08-03 [2] CRAN (R 4.2.1)\n   Matrix              1.4-1      2022-03-23 [2] CRAN (R 4.2.1)\n P memoise             2.0.1      2021-11-26 [?] CRAN (R 4.2.1)\n   mime                0.12       2021-09-28 [2] CRAN (R 4.2.1)\n   miniUI              0.1.1.1    2018-05-18 [2] CRAN (R 4.2.1)\n P ModelMetrics        1.2.2.2    2020-03-17 [?] CRAN (R 4.2.1)\n   munsell             0.5.0      2018-06-12 [2] CRAN (R 4.2.1)\n   nlme                3.1-159    2022-08-09 [2] CRAN (R 4.2.1)\n P nnet                7.3-18     2022-09-28 [?] CRAN (R 4.2.1)\n   parallelly          1.32.1     2022-07-21 [2] CRAN (R 4.2.1)\n   pillar              1.8.1      2022-08-19 [2] CRAN (R 4.2.1)\n   pkgbuild            1.3.1      2021-12-20 [2] CRAN (R 4.2.1)\n   pkgconfig           2.0.3      2019-09-22 [2] CRAN (R 4.2.1)\n   pkgload             1.3.0      2022-06-27 [2] CRAN (R 4.2.1)\n P pls                 2.8-1      2022-07-16 [?] CRAN (R 4.2.1)\n   plyr                1.8.7      2022-03-24 [2] CRAN (R 4.2.1)\n   prettyunits         1.1.1      2020-01-24 [2] CRAN (R 4.2.1)\n P pROC                1.18.0     2021-09-03 [?] CRAN (R 4.2.1)\n   processx            3.7.0      2022-07-07 [2] CRAN (R 4.2.1)\n P prodlim             2019.11.13 2019-11-17 [?] CRAN (R 4.2.1)\n   profvis             0.3.7      2020-11-02 [2] CRAN (R 4.2.1)\n   promises            1.2.0.1    2021-02-11 [2] CRAN (R 4.2.1)\n   ps                  1.7.1      2022-06-18 [2] CRAN (R 4.2.1)\n   purrr               0.3.4      2020-04-17 [2] CRAN (R 4.2.1)\n P R6                  2.5.1      2021-08-19 [?] CRAN (R 4.2.1)\n P randomForest      * 4.7-1.1    2022-05-23 [?] CRAN (R 4.2.1)\n   Rcpp                1.0.9      2022-07-08 [2] CRAN (R 4.2.1)\n P recipes             1.0.3      2022-11-09 [?] CRAN (R 4.2.1)\n   remotes             2.4.2      2021-11-30 [2] CRAN (R 4.2.1)\n   renv                0.15.5     2022-05-26 [1] CRAN (R 4.2.1)\n P reshape2            1.4.4      2020-04-09 [?] CRAN (R 4.2.1)\n P rlang               1.0.6      2022-09-24 [?] CRAN (R 4.2.1)\n P rmarkdown           2.16       2022-08-24 [?] CRAN (R 4.2.1)\n   robustbase        * 0.95-0     2022-04-02 [2] CRAN (R 4.2.1)\n P ROCR              * 1.0-11     2020-05-02 [?] CRAN (R 4.2.1)\n P rpart               4.1.19     2022-10-21 [?] CRAN (R 4.2.1)\n   rstudioapi          0.14       2022-08-22 [2] CRAN (R 4.2.1)\n   scales              1.2.1      2022-08-20 [2] CRAN (R 4.2.1)\n   sessioninfo         1.2.2      2021-12-06 [2] CRAN (R 4.2.1)\n   shiny               1.7.2      2022-07-19 [2] CRAN (R 4.2.1)\n P stringi             1.7.8      2022-07-11 [?] CRAN (R 4.2.1)\n P stringr             1.4.1      2022-08-20 [?] CRAN (R 4.2.1)\n P survival            3.4-0      2022-08-09 [?] CRAN (R 4.2.1)\n   tibble              3.1.8      2022-07-22 [2] CRAN (R 4.2.1)\n P tidyselect          1.2.0      2022-10-10 [?] CRAN (R 4.2.1)\n P timeDate            4021.106   2022-09-30 [?] CRAN (R 4.2.1)\n   urlchecker          1.0.1      2021-11-30 [2] CRAN (R 4.2.1)\n   usethis             2.1.6      2022-05-25 [2] CRAN (R 4.2.1)\n   utf8                1.2.2      2021-07-24 [2] CRAN (R 4.2.1)\n P vctrs               0.5.1      2022-11-16 [?] CRAN (R 4.2.1)\n   withr               2.5.0      2022-03-03 [2] CRAN (R 4.2.1)\n P xfun                0.33       2022-09-12 [?] CRAN (R 4.2.1)\n   xtable              1.8-4      2019-04-21 [2] CRAN (R 4.2.1)\n P yaml                2.3.5      2022-02-21 [?] CRAN (R 4.2.1)\n\n [1] /home/rmflight/Projects/personal/researchblog_quarto/renv/library/R-4.2/x86_64-pc-linux-gnu\n [2] /rmflight_stuff/software/R-4.2.1/library\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}