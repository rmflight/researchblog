{
  "hash": "c5af0cb5b7f360f6507ad5f7dc98bbe9",
  "result": {
    "markdown": "---\ntitle: \"Recreating Correlation Values from Another Manuscript\"\nsubtitle: |\n  Documenting my journey trying to recreate some correlations calculated in another manuscript.\ndate: 2022-01-15\ncategories: [reproducibility, correlation]\nbibliography: refs.bib\n---\n\n\n\n## Background\n\nI've been working on a manuscript for our newish correlation method, information-content-informed Kendall-tau (ICI-Kendalltau, package currently at [@icikendalltau]).\nAs part of that manuscript, we wanted to highlight how well the correlation measure detects outlier samples.\nTwo datasets we are using for that aspect are public, one from The Cancer Genome Atlas and the other from the Barton group.\nThe Barton group and collaborators produced a highly replicated RNAseq yeast dataset, 48 replicates in two conditions, and have used it in various analyses [@Gierlinski2015; @Schurch2016].\n\n## So What\n\nFor my manuscript, ideally I want to be able to comment on the outliers found in one of the Barton group manuscripts [@Gierlinski2015].\nTo do that, I need access to one of:\n\n* the sample-sample correlations themselves,\n* the counts from each sample\n* or be able to recreate counts from each sample.\n\nSeveral years ago I was asking about this dataset on twitter when I was using it for another project, because the project ID in the SRA didn't have a mapping to condition.\nDr. Geoff Barton pointed me to a metadata file available on figshare [@Barton2015].\nThis allowed me to generate read counts across biological replicates (which is what I was interested in at the time).\nHowever, if I had poked around the figshare project a bit more, I would have likely seen both of the files with read counts for each replicate already available [@SNF2Barton2015; @WTBarton2015].\nIt really sucks that I hadn't noticed these other files years ago, it would have saved me the effort in remapping and generating gene counts myself, as well as getting really weird correlation values compared to [@Gierlinski2015].\n\n## My Correlations\n\nFrom the demultiplexed read data and the metadata file I found several years ago, I had run RNA-seq mapping software to generate read counts for each sample in each lane, and summed them across lanes.\nI know how to do these kinds of things, even though I will be the first to admit it is **not** my personal bread and butter analysis (I normally get involved after count generation).\nWith the (few) hints from the manuscript about how [@Gierlinski2015] did the correlation calculation (see more below), I don't get anything close to the range of median correlation values within each class of samples.\nWhich I thought was very, very weird.\nI implemented a variety of transformation methods and inclusion of missing values for my correlation calculations:\n\n* logged values (log and log1p)\n* raw values\n* removal and inclusion of missing (0) values\n\nEven with all of these variations, I could not come close to the same values I found presented in their manuscript.\nHere, in panel (a) of Figure 2 from [@Gierlinski2015], the median correlations range from 0.7 to 1.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Figure 2(a) from [@Gierlinski2015]. Identifying bad RNA-seq replicates in the WT (left) and Δsnf2 (right) data. The top three panels (a–c) show individual criteria for identifying ‘bad’ replicates which are combined into a quality score (d) in order to identify ‘bad’ replicates in each condition. The identified ‘bad’ replicates are shown as numbered points in each panel. The individual criteria are (a) median correlation coefficient, ri∼⁠, for each replicate i against all other replicates, (b) outlier fraction, fi⁠, calculated as a fraction of genes where the given replicate is more than five-trimmed standard deviations from the trimmed mean and (c) median reduced χ2 of pileup depth, χ∼2i⁠, as a measure of the non-uniformity of read distribution within genes (see also Fig. 3)](gierlinski_2015_fig2_cropped.png){class=external}\n:::\n:::\n\n\nMy ranges, however, were much different.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nout_ranges = readRDS(here::here(\"data_files\", \"ranges.rds\"))\nknitr::kable(out_ranges$rmf_ranges, digits = 2, caption = 'RMF median correlation ranges. Which is \"which\" method was used to calculate the correlations.')\n```\n\n::: {.cell-output-display}\nTable: RMF median correlation ranges. Which is \"which\" method was used to calculate the correlations.\n\n|which   |sample_class | high|  low|\n|:-------|:------------|----:|----:|\n|log     |SNF2         | 0.99| 0.96|\n|log     |WT           | 0.97| 0.89|\n|log_no0 |SNF2         | 0.99| 0.95|\n|log_no0 |WT           | 0.96| 0.87|\n|raw     |SNF2         | 0.98| 0.96|\n|raw     |WT           | 0.97| 0.67|\n|raw_no0 |SNF2         | 0.98| 0.96|\n|raw_no0 |WT           | 0.97| 0.67|\n:::\n:::\n\nWe can see in this table, that my median correlation ranges are not even close to what we can see in the figure.\n\nAnd the lowest values and samples didn't seem to be right either.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_ranges$rmf_medians %>%\n  dplyr::filter(which %in% \"raw_no0\") %>%\n  dplyr::group_by(sample_class) %>%\n  dplyr::arrange(med_cor) %>%\n  dplyr::slice_head(n = 6) %>%\n  knitr::kable(., digits = 2, caption = \"Lowest median correlation values using my own counts.\")\n```\n\n::: {.cell-output-display}\nTable: Lowest median correlation values using my own counts.\n\n|sample_id | med_cor|sample_class |which   |\n|:---------|-------:|:------------|:-------|\n|SNF2.7    |    0.96|SNF2         |raw_no0 |\n|SNF2.33   |    0.96|SNF2         |raw_no0 |\n|SNF2.27   |    0.96|SNF2         |raw_no0 |\n|SNF2.8    |    0.96|SNF2         |raw_no0 |\n|SNF2.48   |    0.96|SNF2         |raw_no0 |\n|SNF2.23   |    0.97|SNF2         |raw_no0 |\n|WT.32     |    0.67|WT           |raw_no0 |\n|WT.14     |    0.75|WT           |raw_no0 |\n|WT.11     |    0.80|WT           |raw_no0 |\n|WT.13     |    0.83|WT           |raw_no0 |\n|WT.16     |    0.87|WT           |raw_no0 |\n|WT.12     |    0.91|WT           |raw_no0 |\n:::\n:::\n\n\n## Finding the Data\n\nFinally, during the week of 2021-11-25, I happened across another manuscript on this dataset from 2016 [@Schurch2016], that mentions a GitHub repo that lo and behold had copies of the preprocessed data to the level of gene counts per biological replicate [@bartongithub2015].\nAwesome!\n\n## New Correlation\n\nUsing that preprocessed data, I was finally able to get what amounted to identical values of correlation, based on comparing the lowest correlation values with the figure.\nGreat.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::kable(out_ranges$barton, digits = 2, caption = 'Barton median correlation ranges. Which is \"which\" method was used to calculate the correlations.')\n```\n\n::: {.cell-output-display}\nTable: Barton median correlation ranges. Which is \"which\" method was used to calculate the correlations.\n\n|which   |sample_class | high|  low|\n|:-------|:------------|----:|----:|\n|log     |Snf2         | 0.99| 0.95|\n|log     |WT           | 0.99| 0.97|\n|log_no0 |Snf2         | 0.99| 0.91|\n|log_no0 |WT           | 0.99| 0.94|\n|raw     |Snf2         | 1.00| 0.71|\n|raw     |WT           | 0.99| 0.77|\n|raw_no0 |Snf2         | 1.00| 0.71|\n|raw_no0 |WT           | 0.99| 0.77|\n:::\n:::\n\n\nIn this table, for the **raw** correlations, we finally see median correlation ranges that match what is observed in the figure, especially at the low end.\nImportantly, the Snf2 lowest value is lower than the WT lowest value.\nSo I'm pretty sure I'm getting the correct sample-sample correlations now.\n\nAs well, if I look at the lowest sample - sample correlations in each class, the sample IDs match what is in the figure as well, and so do the median sample-sample correlations for those samples!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_ranges$barton_medians %>%\n  dplyr::filter(which %in% \"raw_no0\") %>%\n  dplyr::group_by(sample_class) %>%\n  dplyr::arrange(med_cor) %>%\n  dplyr::slice_head(n = 6) %>%\n  knitr::kable(., digits = 2, caption = \"Lowest median correlation values using Barton counts.\")\n```\n\n::: {.cell-output-display}\nTable: Lowest median correlation values using Barton counts.\n\n|sample_id | med_cor|sample_class |which   |\n|:---------|-------:|:------------|:-------|\n|Snf2.06   |    0.71|Snf2         |raw_no0 |\n|Snf2.13   |    0.83|Snf2         |raw_no0 |\n|Snf2.35   |    0.96|Snf2         |raw_no0 |\n|Snf2.10   |    0.99|Snf2         |raw_no0 |\n|Snf2.25   |    0.99|Snf2         |raw_no0 |\n|Snf2.24   |    0.99|Snf2         |raw_no0 |\n|WT.21     |    0.77|WT           |raw_no0 |\n|WT.25     |    0.86|WT           |raw_no0 |\n|WT.28     |    0.89|WT           |raw_no0 |\n|WT.22     |    0.94|WT           |raw_no0 |\n|WT.17     |    0.98|WT           |raw_no0 |\n|WT.36     |    0.98|WT           |raw_no0 |\n:::\n:::\n\n\n## Why No Log-Transformation?\n\nOne interesting thing about having the correct correlations is discovering that Gierlinski et al didn't use log-transformed data in their Pearson correlation calculations.\nThis seems unusual to me.\nAll my career in -omics, the one thing I've had drilled into me is that doing a linear correlation on data that has proportional error component or variance is a **very bad** idea.\nProportional error or variance means the variance increases with increasing mean values, which is definitely true of these count data.\n\nIf I had to guess **why** log-transformed values weren't used, I think it is because of the analysis in the replicate paper about how well the un-transformed values fit a normal distribution vs a log-normal distribution.\nThat figure and caption are provided here for reference.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Figure 5 from [@Gierlinski2015]. Goodness-of-fit test results for normal (top panels), log-normal (middle panels) and negative binomial (bottom panels) distributions. Each panel shows the test P-value versus the mean count across replicates. Each dot represents equal-count normalized data from one gene. Panels on the left (a, b, e, f, i, j) show clean data with bad replicates rejected (42 and 44 replicates remaining in WT and Δsnf2, respectively). Panels on the right (c, d, g, h, k, l) show all available data (48 replicates in each condition). Due to the number of bootstraps performed, P-values for the negative-binomial test are limited to ∼10−2. Due to numerical precision of the software library used, P-values from the normal and log-normal tests are limited to ∼10−16. Below these limits data points are marked in orange (light gray in black and white) at the bottom of each panel. Horizontal lines show the Benjamini–Hochberg limit corresponding to the significance of 0.05 for the given dataset. The numbers in the right bottom corner of each panel indicate the number of genes with P-values below the significance limit and the total number of genes](gierlinski_2015_fig5.jpeg){class=external}\n:::\n:::\n\n\nJust so you can see the difference using raw and log-space makes (besides the values in the above table), here are two of the replicate samples plotted against each other in raw and log-transformed values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork)\nlibrary(ggplot2)\ntheme_set(cowplot::theme_cowplot())\n\nraw_plot = readRDS(here::here(\"data_files\", \"raw_plot.rds\"))\nraw_p2 = raw_plot + \n  coord_equal() +\n  geom_abline(slope = 1, color = \"red\")\nlog_plot = readRDS(here::here(\"data_files\", \"log_plot.rds\"))\nlog_p2 = log_plot +\n  coord_equal() +\n  geom_abline(slope = 1, color = \"red\")\nraw_p2 + log_p2\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/raw_log-1.png){width=672}\n:::\n:::\n\n\nWe can summarize this behavior across **all** the replicate samples, looking at standard deviation (SD) and relative standard deviation (RSD, SD / mean).\nI took the counts from Barton group after removing outliers based on median correlations, and then calculate the mean, SD, and RSD across all replicates in each of Snf2 and WT.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_summary = readRDS(here::here(\"data_files\", \"summary.rds\"))\n\ndata_summary %>%\n  dplyr::filter(!(type %in% \"diff\")) %>%\n  ggplot(aes(x = mean, y = var)) + \n  geom_point() +\n  facet_wrap(~ type, ncol = 1, scales = \"free_y\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 494 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/sd_rsd-1.png){width=480}\n:::\n:::\n\n\nAs we can see in the above figure, SD increases with increasing mean.\nWhich is basically what we observe in the pairwise plot above.\nI'm pretty sure I've been taught **not** to do Pearson correlation on data with this structure.\nInterestingly, the RSD does become rather constant after a certain value in mean expression.\n\n## How Did I Miss the Original Data?\n\nBack to the question of finding the original data, it turns out I just didn't look hard enough at the first place that Geoff Barton sent me to.\nWhen I started poking around the figshare repo's from Geoff and others in the group and following links, it was easy to find a copy of the preprocessed data from each condition.\n\n## Why Was it So Hard to Reproduce Values?\n\nAnother interesting thing about this endeavor was just how hard it was to reproduce the correlations.\n[@Gierlinski2015] was actually light on details of how the data was processed, and how the correlation was done.\nFor example, the manuscript just says \"Pearson correlation\", with basically no other details.\nWere raw counts used or log-transformed?\nWere missing (count of 0) values in either sample removed prior to correlation?\nIt was only when I finally happened across the GitHub repo that I finally got the answers I really needed.\nAnd for some reason, that manuscript doesn't mention the GitHub repo, or the data available on figshare.\nThis points to the importance of citing and linking all the resources for a manuscript (or even a blog post!).\nAnd I'm not trying to knock on Marek, or Geoff, or Nick on this.\nIn my experience, they post open things, and provide lots of data.\nBut this highlights just how hard it becomes to recreate something if even a little piece of the data is missing.\n\n## Data Files and Scripts\n\nThe rds files and a processing script to calculate the correlations and generate the plots are all available on the blog directory on GitHub [@flight_post].\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}