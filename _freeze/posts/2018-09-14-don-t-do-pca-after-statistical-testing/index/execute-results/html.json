{
  "hash": "9dfbcf0d2dcd2a1788c3954c459a9714",
  "result": {
    "markdown": "---\ntitle: \"Don't do PCA After Statistical Testing!\"\ndate: '2018-09-14'\ncategories: [pca, bioinformatics, R, t-test]\nsubtitle: |\n  You might be tempted to do PCA after a statistical test. Read more to discover why this is a bad idea.\n---\n\n\n\n\n## TL;DR\n\nIf you do a statistical test **before** a dimensional reduction method like\nPCA, the highest source of variance is likely to be whatever you tested\nstatistically.\n\n## Wait, Why??\n\nLet me describe the situation. You've done an `-omics` level analysis on your\nsystem of interest. You run a t-test (or ANOVA, etc) on each of the features\nin your data (gene, protein, metabolite, etc). Filter down to those things that\nwere statistically significant, and then finally, you decide to look at the data\nusing a dimensionality reduction method such as *principal components analysis*\n(PCA) so you can **see** what is going on.\n\nI have seen this published at least once (in a Diabetes metabolomics paper,\nif anyone knows it, please send it to me so I can link it), and have seen\ncollaborators do this after coaching from others in non-statistical departments.\n\n## The Problem\n\nThe problem is that PCA is just looking at either feature-feature covariances\nor sample-sample covariances. If you have trimmed the data to those things that\nhave statistically significant differences, then you have completely modified\nthe covariances, and PCA is likely to pick up on that.\n\n## An Example\n\nLet's actually do an example where there are no differences initially, and\nthen see if we can introduce an artificial difference.\n\n### Random Data\n\nWe start with completely random data, 10000 features, and 100 samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_feat = 10000\nn_sample = 100\nrandom_data = matrix(rnorm(n_feat * n_sample), nrow = n_feat, ncol = n_sample)\n```\n:::\n\n\nNow we will do a t-test on each row, taking the first 50 samples as class 1\nand the other 50 samples as class 2.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt_test_res = purrr::map_df(seq(1, nrow(random_data)), function(in_row){\n  tidy(t.test(random_data[in_row, 1:50], random_data[in_row, 51:100]))\n})\n```\n:::\n\n\nHow many are significant at a p-value of 0.05?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter(t_test_res, p.value <= 0.05) %>% dim()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 514  10\n```\n:::\n:::\n\n\nObviously, these are false positives, but they are enough for us to illustrate\nthe problem.\n\nFirst, lets do PCA on the whole data set of 10000 features.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_classes = data.frame(class = c(rep(\"A\", 50), rep(\"B\", 50)))\n\nall_pca = prcomp(t(random_data), center = TRUE, scale. = FALSE)\npca_scores = cbind(as.data.frame(all_pca$x), sample_classes)\nggplot(pca_scores, aes(x = PC1, y = PC2, color = class)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/all_pca-1.png){width=672}\n:::\n:::\n\n\nObviously, there is no difference in the groups, and the % explained variance\nis very low.\n\nSecond, lets do it on just those things that were significant:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsig_pca = prcomp(t(random_data[which(t_test_res$p.value <= 0.05), ]), center = TRUE,\n                 scale. = FALSE)\nsig_scores = cbind(as.data.frame(sig_pca$x), sample_classes)\n\nggplot(sig_scores, aes(x = PC1, y = PC2, color = class)) + \n  geom_point() +\n  theme(legend.position = c(0.5, 0.5))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/sig_pca-1.png){width=672}\n:::\n:::\n\n\nAnd look at that! We have separation of the two groups! But ...., this is\ncompletely random data, that didn't have any separation, **until we did the\nstatistical test**!\n\n## Take Away\n\nBe careful of the order in which you do things. If you want to do dimensionality\nreduction to look for issues with the samples, then do that **before** any statistical\ntesting on the individual features.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}